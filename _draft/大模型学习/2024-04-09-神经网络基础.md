---
title: 神经网络基础
author: Jing
date: 2024-04-09
tags:
  - 神经网络基础
---
# 1. Layer Normalization

`torch.nn.functional` 中的 layer_norm
- 代码
```python
def layer_norm(
    input: Tensor,
    normalized_shape: List[int],
    weight: Optional[Tensor] = None,
    bias: Optional[Tensor] = None,
    eps: float = 1e-5,
) -> Tensor:
    r"""Applies Layer Normalization for last certain number of dimensions.

    See :class:`~torch.nn.LayerNorm` for details.
    """
    if has_torch_function_variadic(input, weight, bias):
        return handle_torch_function(
            layer_norm, (input, weight, bias), input, normalized_shape, weight=weight, bias=bias, eps=eps
        )
    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
```
- `torch.nn.modules.normalization` 的 LayerNorm forward 代码
```python
class LayerNorm(Module):
	……
	def forward(self, input: Tensor) -> Tensor:
		return F.layer_norm(
			input, self.normalized_shape, self.weight, self.bias, self.eps)
```
- 报错：
```text
RuntimeError: Given normalized_shape=[384], expected input with shape [*, 384], but got input of size[256]
```
- 解决方案
