---
title: PromptKD
author: Jing
date: 2024-03-29
tags:
  - CVPR2024
  - Prompt_learning
  - 知识蒸馏
---
[论文](https://arxiv.org/pdf/2403.02781.pdf)
[作者博客](https://zhuanlan.zhihu.com/p/684269963)
# 1. 领域
> `Prompt learning` 是一种可以将大的预训练模型，不需要完全重新训练原始模型，而迁移到下游任务的一种技术。  
> `Soft prompts`

`Prompt learning`、知识蒸馏
在做 `Prompt engineering`（`Prompt learning`），基于大模型强大的泛化能力，更好优化特定领域下游任务的一种方法。  
# 2. 前人做的
`Prompt learning` 使用稀缺的标记数据获取有效 `prompt`
# 3. Prompt learning 和 Zero-shot learning 
`Prompt learning` 分类：  
- `Text-based`
- `Visual-based`  
- `Text-based and visual-based ` 
`Zero-shot learning` 分类：  
- `Inductive ZSL`：不利用未见过类别的任何信息，去预测未见过类别的标签  
- `Transductive ZSL`：利用未见过类别的无标签数据，去预测未见过类别的标签  
# 4. 创新点
![[PromptKD.png]]
提出 `unsupervised domain prompt distillation framework`  
- 预训练一个 `CLIP` 教师模型；  
	- 使用**少量的标签数据**预训练  
- 将文本特征存储为类向量（师生共享）；  
	- `save well-trained teacher text features as class vectors for student distillation`  
- 使用 KL 散度对齐师生图像编码部分（`prompt`）  
	- 训练学生模型时，使用的是完整的未标记数据集（包含可见和未见类别的所有样本）  
- 使用教师的 `text feature` 和学生蒸馏的` img encoder`，推理得到结果  
# 5. 感想
`PromptKD `使用了知识蒸馏去做 `Prompt learning`，有关 `Prompt learning` 还是有很多没有看懂。